# ğŸ”Ÿ é€²éšæ‡‰ç”¨èˆ‡å°ˆæ¡ˆç¤ºç¯„

## ğŸ¯ Keras çš„é€²éšæ‡‰ç”¨

ç¾åœ¨æˆ‘å€‘å·²ç¶“å­¸æœƒäº† Keras çš„åŸºç¤èˆ‡æ ¸å¿ƒæ¦‚å¿µï¼Œæ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡æ¢ç´¢ Keras åœ¨å¯¦éš›å°ˆæ¡ˆä¸­çš„æ‡‰ç”¨ã€‚

âœ… **é€²éšæ‡‰ç”¨é ˜åŸŸ**ï¼š

1. **å½±åƒåˆ†é¡ï¼ˆImage Classificationï¼‰**
2. **ç‰©ä»¶åµæ¸¬ï¼ˆObject Detectionï¼‰**
3. **è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰**
4. **æ™‚é–“åºåˆ—é æ¸¬ï¼ˆTime Series Forecastingï¼‰**
5. **ç”Ÿæˆå°æŠ—ç¶²è·¯ï¼ˆGANsï¼‰**

---

## **âœ… å½±åƒåˆ†é¡å°ˆæ¡ˆç¤ºç¯„ï¼ˆImage Classificationï¼‰**

æˆ‘å€‘å¯ä»¥ä½¿ç”¨ Keras ä¾†å»ºç«‹ä¸€å€‹å½±åƒåˆ†é¡æ¨¡å‹ï¼Œä¾‹å¦‚ CIFAR-10 æ•¸æ“šé›†ã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.datasets import cifar10

# è¼‰å…¥ CIFAR-10 æ•¸æ“šé›†
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# æ­£è¦åŒ–æ•¸æ“š
x_train, x_test = x_train / 255.0, x_test / 255.0

# å»ºç«‹ CNN æ¨¡å‹
model = keras.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# ç·¨è­¯æ¨¡å‹
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# è¨“ç·´æ¨¡å‹
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))
```

âœ… **é€™æ˜¯ä¸€å€‹é©ç”¨æ–¼å½±åƒåˆ†é¡çš„ CNN æ¨¡å‹ï¼Œé©åˆæ‡‰ç”¨æ–¼ç‰©ä»¶è¾¨è­˜ç­‰ä»»å‹™ã€‚**

---

## **âœ… NLP æ‡‰ç”¨ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰**

Keras ä¹Ÿå¯ä»¥ç”¨æ–¼ **è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰**ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†æã€‚

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# å»ºç«‹è¨“ç·´æ•¸æ“š
documents = ["I love this movie!", "This film is terrible.", "Amazing story and characters."]
labels = [1, 0, 1]  # 1: æ­£é¢æƒ…ç·’, 0: è² é¢æƒ…ç·’

# Tokenization
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(documents)
sequences = tokenizer.texts_to_sequences(documents)

# å¡«å……åºåˆ—
padded_sequences = pad_sequences(sequences, maxlen=10)
```

âœ… **é€™æ¨£å¯ä»¥å°‡æ–‡å­—è½‰æ›ç‚ºæ•¸å€¼è¡¨ç¤ºï¼Œä»¥ä¾¿è¼¸å…¥åˆ° NLP æ¨¡å‹ä¸­é€²è¡Œè¨“ç·´ã€‚**

---

## ğŸ“ **ç¸½çµ**

| **æ‡‰ç”¨é ˜åŸŸ** | **ç¯„ä¾‹** |
|----------|--------|
| **å½±åƒåˆ†é¡** | `CNN` è™•ç† CIFAR-10 æ•¸æ“šé›† |
| **è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰** | æ–‡å­— Tokenization èˆ‡æƒ…æ„Ÿåˆ†æ |
| **æ™‚é–“åºåˆ—é æ¸¬** | `LSTM` è™•ç†é‡‘èæ•¸æ“šé æ¸¬ |
| **ç‰©ä»¶åµæ¸¬** | `YOLO` é€²è¡Œå³æ™‚å½±åƒåˆ†æ |
| **ç”Ÿæˆå°æŠ—ç¶²è·¯ï¼ˆGANsï¼‰** | å½±åƒç”Ÿæˆèˆ‡é¢¨æ ¼è½‰æ› |

ğŸš€ **æ­å–œä½ ï¼ä½ å·²ç¶“å®Œæˆäº† Keras æ•™å­¸èª²ç¨‹ï¼ç¾åœ¨ä½ å¯ä»¥ä½¿ç”¨ Keras æ§‹å»ºè‡ªå·±çš„æ·±åº¦å­¸ç¿’å°ˆæ¡ˆäº†ï¼ğŸ‰** ğŸ˜Š

