# 5ï¸âƒ£ Keras ä¸­çš„æ•¸æ“šè™•ç†

## ğŸ¯ ç‚ºä»€éº¼æ•¸æ“šè™•ç†å¾ˆé‡è¦ï¼Ÿ

åœ¨è¨“ç·´æ·±åº¦å­¸ç¿’æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘å€‘é€šå¸¸éœ€è¦å°æ•¸æ“šé€²è¡Œ **é è™•ç†ï¼ˆPreprocessingï¼‰**ï¼Œä»¥ç¢ºä¿æ¨¡å‹å¯ä»¥æœ‰æ•ˆå­¸ç¿’ã€‚

âœ… **æ•¸æ“šè™•ç†çš„é—œéµæ­¥é©Ÿ**ï¼š

1. **æ•¸æ“šæ¨™æº–åŒ–ï¼ˆNormalizationï¼‰**
2. **æ•¸æ“šå¢å¼·ï¼ˆData Augmentationï¼‰**
3. **æ•¸æ“šåˆ†å‰²ï¼ˆTrain / Validation / Test Splitï¼‰**
4. **è™•ç†å½±åƒæ•¸æ“šï¼ˆImage Processingï¼‰**
5. **è™•ç†æ–‡æœ¬æ•¸æ“šï¼ˆText Tokenizationï¼‰**

---

## **âœ… æ•¸æ“šæ¨™æº–åŒ–ï¼ˆNormalizationï¼‰**

æ•¸æ“šæ¨™æº–åŒ–å¯ä»¥è®“æ‰€æœ‰ç‰¹å¾µçš„æ•¸å€¼ç¯„åœä¸€è‡´ï¼Œæé«˜æ¨¡å‹è¨“ç·´çš„ç©©å®šæ€§ã€‚

```python
import numpy as np

# ç”Ÿæˆéš¨æ©Ÿæ•¸æ“šï¼ˆæ¨¡æ“¬ç‰¹å¾µï¼‰
data = np.random.randint(0, 255, (100, 10), dtype=np.float32)

# æ¨™æº–åŒ–åˆ° [0, 1] å€é–“
data_normalized = data / 255.0
```

âœ… **å¸¸è¦‹çš„æ¨™æº–åŒ–æ–¹å¼åŒ…æ‹¬ Min-Max Scalingï¼ˆæ­¸ä¸€åŒ–åˆ° 0-1ï¼‰èˆ‡ Z-score æ¨™æº–åŒ–ï¼ˆå‡å€¼ 0ï¼Œæ¨™æº–å·® 1ï¼‰ã€‚**

---

## **âœ… æ•¸æ“šå¢å¼·ï¼ˆData Augmentationï¼‰**

åœ¨å½±åƒè™•ç†ä¸­ï¼Œæˆ‘å€‘å¯ä»¥é€šé **æ—‹è½‰ã€ç¿»è½‰ã€ç¸®æ”¾** ä¾†å¢åŠ æ•¸æ“šå¤šæ¨£æ€§ã€‚

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=30, 
    width_shift_range=0.2, 
    height_shift_range=0.2, 
    horizontal_flip=True
)
```

âœ… **é€™æœ‰åŠ©æ–¼æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢éæ“¬åˆï¼ˆOverfittingï¼‰ã€‚**

---

## **âœ… è¨“ç·´æ•¸æ“šèˆ‡æ¸¬è©¦æ•¸æ“šåˆ†å‰²**

ä¸€èˆ¬ä¾†èªªï¼Œæ•¸æ“šæ‡‰è©²æŒ‰ç…§ **80% è¨“ç·´ï¼ˆTrainingï¼‰ã€10% é©—è­‰ï¼ˆValidationï¼‰ã€10% æ¸¬è©¦ï¼ˆTestingï¼‰** ä¾†åˆ†å‰²ã€‚

```python
from sklearn.model_selection import train_test_split

# ç”Ÿæˆéš¨æ©Ÿæ•¸æ“š
data = np.random.rand(1000, 10)
labels = np.random.randint(0, 2, size=(1000,))

# åˆ†å‰²æ•¸æ“š
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)
```

âœ… **ç¢ºä¿æ¸¬è©¦æ•¸æ“šä¸è¢«ç”¨æ–¼è¨“ç·´ï¼Œä»¥æª¢é©—æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚**

---

## **âœ… å½±åƒæ•¸æ“šè™•ç†ï¼ˆImage Processingï¼‰**

åœ¨ Keras ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ `ImageDataGenerator` ä¾†è®€å–èˆ‡å¢å¼·å½±åƒæ•¸æ“šã€‚

```python
train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    'dataset/train',
    target_size=(128, 128),
    batch_size=32,
    class_mode='binary'
)
```

âœ… **é€™å°‡è‡ªå‹•è®€å–å½±åƒè³‡æ–™å¤¾ï¼Œä¸¦å°‡å½±åƒç¸®æ”¾åˆ° [0,1] å€é–“ã€‚**

---

## **âœ… è™•ç†æ–‡æœ¬æ•¸æ“šï¼ˆText Tokenizationï¼‰**

åœ¨ NLP é ˜åŸŸï¼Œæˆ‘å€‘éœ€è¦å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å­—æ ¼å¼æ‰èƒ½è¼¸å…¥åˆ°ç¥ç¶“ç¶²è·¯ã€‚

```python
from tensorflow.keras.preprocessing.text import Tokenizer

# æ–‡å­—æ•¸æ“š
documents = ["I love deep learning", "Keras makes it easy"]

# å»ºç«‹ Tokenizer
tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(documents)

# è½‰æ›ç‚ºæ•¸å­—åºåˆ—
sequences = tokenizer.texts_to_sequences(documents)
print(sequences)
```

âœ… **é€™å¯ä»¥å°‡å¥å­è½‰æ›ç‚ºæ•¸å­—åºåˆ—ï¼Œä»¥ä¾¿ç¥ç¶“ç¶²è·¯è™•ç†ã€‚**

---

## ğŸ“ **ç¸½çµ**

| **åŠŸèƒ½** | **èªæ³•** |
|----------|--------|
| **æ•¸æ“šæ¨™æº–åŒ–** | `data / 255.0` |
| **å½±åƒå¢å¼·** | `ImageDataGenerator(rotation_range=30, horizontal_flip=True)` |
| **æ•¸æ“šåˆ†å‰²** | `train_test_split(data, labels, test_size=0.2)` |
| **å½±åƒæ•¸æ“šè™•ç†** | `flow_from_directory('dataset/train', target_size=(128,128))` |
| **æ–‡æœ¬ Tokenization** | `texts_to_sequences(documents)` |

ğŸš€ **ç¾åœ¨ä½ å·²ç¶“å­¸æœƒå¦‚ä½•åœ¨ Keras ä¸­é€²è¡Œæ•¸æ“šé è™•ç†ï¼æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ Keras åœ¨ CNNï¼ˆå·ç©ç¥ç¶“ç¶²è·¯ï¼‰ä¸­çš„æ‡‰ç”¨ï¼** ğŸ˜Š

